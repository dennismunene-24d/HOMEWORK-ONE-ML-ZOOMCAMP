{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62cad35a-fd6e-4e97-9079-7e957d14bbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "url = \"https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip\"\n",
    "response = requests.get(url)\n",
    "z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "z.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa122d24-340c-4c5d-a736-cf83d51ecfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7616bb1-5bb3-4c09-b00e-975814ece48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b84da-fb12-4618-8843-05fd477a1abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b6c22ed-7ee0-4466-aefd-34debd71e85b",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "The dataset contains around 1000 images of hairs in the separate folders \n",
    "for training and test sets. \n",
    "\n",
    "### Reproducibility\n",
    "\n",
    "Reproducibility in deep learning is a multifaceted challenge that requires attention \n",
    "to both software and hardware details. In some cases, we can't guarantee exactly the same results during the same experiment runs.\n",
    "\n",
    "Therefore, in this homework we suggest to set the random number seed generators by:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "```\n",
    "\n",
    "Also, use PyTorch of version 2.8.0 (that's the one in Colab).\n",
    "\n",
    "### Model\n",
    "\n",
    "For this homework we will use Convolutional Neural Network (CNN). We'll use PyTorch.\n",
    "\n",
    "You need to develop the model with following structure:\n",
    "\n",
    "* The shape for input should be `(3, 200, 200)` (channels first format in PyTorch)\n",
    "* Next, create a convolutional layer (`nn.Conv2d`):\n",
    "    * Use 32 filters (output channels)\n",
    "    * Kernel size should be `(3, 3)` (that's the size of the filter)\n",
    "    * Use `'relu'` as activation \n",
    "* Reduce the size of the feature map with max pooling (`nn.MaxPool2d`)\n",
    "    * Set the pooling size to `(2, 2)`\n",
    "* Turn the multi-dimensional result into vectors using `flatten` or `view`\n",
    "* Next, add a `nn.Linear` layer with 64 neurons and `'relu'` activation\n",
    "* Finally, create the `nn.Linear` layer with 1 neuron - this will be the output\n",
    "    * The output layer should have an activation - use the appropriate activation for the binary classification case\n",
    "\n",
    "As optimizer use `torch.optim.SGD` with the following parameters:\n",
    "\n",
    "* `torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4fbb13c8-73db-459e-b9ed-48a4bc61d968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06842f03-084b-419c-a51c-34c208b7f742",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Which loss function you will use?\n",
    "\n",
    "* `nn.MSELoss()`\n",
    "* `nn.BCEWithLogitsLoss()`\n",
    "* `nn.CrossEntropyLoss()`\n",
    "* `nn.CosineEmbeddingLoss()`\n",
    "\n",
    "## Answer   `nn.BCEWithLogitsLoss()`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69d52122-2a21-4b1d-8738-c8f218ab5916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HairCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 32, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(32 * 99 * 99, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4cd2d8f9-a983-48ac-ac3e-b7bcc7ca5f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "HairModel                                [1, 1]                    --\n",
       "├─Conv2d: 1-1                            [1, 32, 198, 198]         896\n",
       "├─MaxPool2d: 1-2                         [1, 32, 99, 99]           --\n",
       "├─Linear: 1-3                            [1, 64]                   20,072,512\n",
       "├─Linear: 1-4                            [1, 1]                    65\n",
       "==========================================================================================\n",
       "Total params: 20,073,473\n",
       "Trainable params: 20,073,473\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 55.20\n",
       "==========================================================================================\n",
       "Input size (MB): 0.48\n",
       "Forward/backward pass size (MB): 10.04\n",
       "Params size (MB): 80.29\n",
       "Estimated Total Size (MB): 90.81\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = HairModel()  # stays on CPU\n",
    "summary(model, input_size=(1, 3, 200, 200))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c9be3-6ca8-44de-a87f-d952c1f1d7f1",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "What's the total number of parameters of the model? You can use `torchsummary` or count manually. \n",
    "\n",
    "In PyTorch, you can find the total number of parameters using:\n",
    "\n",
    "```python\n",
    "# Option 1: Using torchsummary (install with: pip install torchsummary)\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(3, 200, 200))\n",
    "\n",
    "# Option 2: Manual counting\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "```\n",
    "\n",
    "* 896 \n",
    "* 11214912\n",
    "* 15896912\n",
    "* 20073473"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a5dbad-6a1b-4a30-b804-3a5360dc9801",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "What is the median of training accuracy for all the epochs for this model?\n",
    "\n",
    "* 0.05\n",
    "* 0.12\n",
    "* 0.40\n",
    "* 0.84 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14b71057-b514-4400-8523-4469fae03fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "208148d1-fd83-479e-a5d7-d6b2f2a521aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"C:/Users/HP/Desktop/Machine_Learning_Zoomcamp/machine-learning-zoomcamp-master/08-deep-learning/data/data\"\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=os.path.join(data_dir, \"train\"), transform=train_transforms)\n",
    "test_dataset = datasets.ImageFolder(root=os.path.join(data_dir, \"test\"), transform=test_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7145dc7d-dac9-4d08-be9f-bbd3d1a95832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Define sizes for train and validation\n",
    "val_size = int(0.2 * len(train_dataset))  # 20% for validation\n",
    "train_size = len(train_dataset) - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, validation_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Now create the data loaders\n",
    "batch_size = 20\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f245038-cad1-4fff-b27a-b9a4bfeff7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.002, momentum=0.8)\n",
    "import torch.nn as nn\n",
    "\n",
    "# Binary classification loss\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef547d8a-b0e6-4d51-946e-bba984825843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6472, Acc: 0.6516, Val Loss: 0.6059, Val Acc: 0.6625\n",
      "Epoch 2/10, Loss: 0.5449, Acc: 0.7063, Val Loss: 0.6885, Val Acc: 0.5813\n",
      "Epoch 3/10, Loss: 0.4883, Acc: 0.7500, Val Loss: 0.5729, Val Acc: 0.7063\n",
      "Epoch 4/10, Loss: 0.4191, Acc: 0.7953, Val Loss: 0.6047, Val Acc: 0.6813\n",
      "Epoch 5/10, Loss: 0.3951, Acc: 0.8094, Val Loss: 0.6391, Val Acc: 0.6500\n",
      "Epoch 6/10, Loss: 0.2811, Acc: 0.8906, Val Loss: 0.7502, Val Acc: 0.6687\n",
      "Epoch 7/10, Loss: 0.2599, Acc: 0.8984, Val Loss: 0.6435, Val Acc: 0.6813\n",
      "Epoch 8/10, Loss: 0.2022, Acc: 0.9141, Val Loss: 0.6747, Val Acc: 0.7312\n",
      "Epoch 9/10, Loss: 0.1367, Acc: 0.9594, Val Loss: 0.6746, Val Acc: 0.7438\n",
      "Epoch 10/10, Loss: 0.0859, Acc: 0.9844, Val Loss: 0.8910, Val Acc: 0.6875\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.float().unsqueeze(1) # Ensure labels are float and have shape (batch_size, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        # For binary classification with BCEWithLogitsLoss, apply sigmoid to outputs before thresholding for accuracy\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = correct_train / total_train\n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['acc'].append(epoch_acc)\n",
    "\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_running_loss += loss.item() * images.size(0)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_epoch_loss = val_running_loss / len(validation_dataset)\n",
    "    val_epoch_acc = correct_val / total_val\n",
    "    history['val_loss'].append(val_epoch_loss)\n",
    "    history['val_acc'].append(val_epoch_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de3c9b54-8608-417b-9689-09e2ee955364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median training accuracy: 0.8500\n"
     ]
    }
   ],
   "source": [
    "train_accuracies = history['acc']\n",
    "median_acc = np.median(train_accuracies)\n",
    "print(f\"Median training accuracy: {median_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a89e01-1695-4efd-b3b7-4e8733d1163d",
   "metadata": {},
   "source": [
    "\n",
    "### Question 4\n",
    "\n",
    "What is the standard deviation of training loss for all the epochs for this model?\n",
    "\n",
    "* 0.007\n",
    "* 0.078\n",
    "* 0.171\n",
    "* 1.710"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "898f9a0e-4798-46d3-85a2-7760c4efa78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation of training loss: 0.163\n"
     ]
    }
   ],
   "source": [
    "train_losses = [0.7181, 0.5645, 0.5177, 0.4679, 0.4112, 0.3610, 0.3578, 0.2131, 0.2322, 0.1697]\n",
    "std_loss = np.std(train_losses)\n",
    "print(f\"Standard deviation of training loss: {std_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc8fe89-c335-487b-8b5a-b65af87ccf28",
   "metadata": {},
   "source": [
    "### Question 5 \n",
    "\n",
    "Let's train our model for 10 more epochs using the same code as previously.\n",
    "\n",
    "> **Note:** make sure you don't re-create the model.\n",
    "> we want to continue training the model we already started training.\n",
    "\n",
    "What is the mean of test loss for all the epochs for the model trained with augmentations?\n",
    "\n",
    "* 0.008\n",
    "* 0.08\n",
    "* 0.88\n",
    "* 8.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bf0f0a11-4d61-403e-9d60-c6989c31f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transforms_aug = transforms.Compose([\n",
    "    transforms.RandomRotation(50),\n",
    "    transforms.RandomResizedCrop(200, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Keep test transforms same as before\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3322c13c-b489-47bc-bc2e-7798928ecdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "data_dir = \"C:/Users/HP/Desktop/Machine_Learning_Zoomcamp/machine-learning-zoomcamp-master/08-deep-learning/data/data\"\n",
    "\n",
    "train_dataset_aug = datasets.ImageFolder(root=os.path.join(data_dir, \"train\"), transform=train_transforms_aug)\n",
    "test_dataset = datasets.ImageFolder(root=os.path.join(data_dir, \"test\"), transform=test_transforms)\n",
    "\n",
    "batch_size = 20\n",
    "train_loader_aug = DataLoader(train_dataset_aug, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ed3df67b-fa82-45f4-9f73-0551f2ffac23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Test Loss: 0.8082 | Test Acc: 0.5622\n",
      "Epoch 2/10 | Test Loss: 0.6907 | Test Acc: 0.6269\n",
      "Epoch 3/10 | Test Loss: 0.6555 | Test Acc: 0.6667\n",
      "Epoch 4/10 | Test Loss: 0.5850 | Test Acc: 0.6617\n",
      "Epoch 5/10 | Test Loss: 0.6139 | Test Acc: 0.6667\n",
      "Epoch 6/10 | Test Loss: 0.6138 | Test Acc: 0.6567\n",
      "Epoch 7/10 | Test Loss: 0.6212 | Test Acc: 0.6866\n",
      "Epoch 8/10 | Test Loss: 0.6033 | Test Acc: 0.6965\n",
      "Epoch 9/10 | Test Loss: 0.5619 | Test Acc: 0.6866\n",
      "Epoch 10/10 | Test Loss: 0.5741 | Test Acc: 0.7015\n",
      "Mean test loss with augmentation: 0.633\n"
     ]
    }
   ],
   "source": [
    "num_epochs_aug = 10  # 10 more epochs\n",
    "\n",
    "test_losses_aug = []\n",
    "test_accuracies_aug = []\n",
    "\n",
    "for epoch in range(num_epochs_aug):\n",
    "    model.train()\n",
    "    for images, labels in train_loader_aug:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.float().unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_test_loss += loss.item() * images.size(0)\n",
    "\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_test_loss = running_test_loss / len(test_dataset)\n",
    "    epoch_test_acc = correct_test / total_test\n",
    "\n",
    "    test_losses_aug.append(epoch_test_loss)\n",
    "    test_accuracies_aug.append(epoch_test_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs_aug} | Test Loss: {epoch_test_loss:.4f} | Test Acc: {epoch_test_acc:.4f}\")\n",
    "\n",
    "# Calculate mean test loss for Question 5\n",
    "mean_test_loss = np.mean(test_losses_aug)\n",
    "print(f\"Mean test loss with augmentation: {mean_test_loss:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bffecf-b1d5-401b-b3d5-4f1710f70089",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "What's the average of test accuracy for the last 5 epochs (from 6 to 10)\n",
    "for the model trained with augmentations?\n",
    "\n",
    "* 0.08\n",
    "* 0.28\n",
    "* 0.68\n",
    "* 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1bd72152-56fc-4af3-b42a-e346e37e2b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test accuracy last 5 epochs: 0.686\n"
     ]
    }
   ],
   "source": [
    "mean_test_acc_last5 = np.mean(test_accuracies_aug[-5:])\n",
    "print(f\"Average test accuracy last 5 epochs: {mean_test_acc_last5:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
